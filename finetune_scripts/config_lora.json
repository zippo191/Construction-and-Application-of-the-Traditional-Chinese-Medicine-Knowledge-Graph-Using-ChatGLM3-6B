{
  "model_name_or_path": "../base_model/chatglm3-6b",
  "output_dir": "./lora_outputs",

  "epochs": 20,
  "learning_rate": 1e-5,
  "batch_size": 16,
  "lora_rank": 8,

  "evaluation_strategy": "steps",
  "eval_steps": 500,
  "save_steps": 1000,
  "logging_steps": 100,
  "gradient_accumulation_steps": 1,
  "mixed_precision": "fp16",

  "report_to": "none"
}
